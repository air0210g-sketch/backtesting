"""
ç›‘æ§æŒ‡æ ‡ç­›é€‰ï¼šä» stock_list è¯»å–è‚¡ç¥¨ï¼ŒåŠ è½½è¿‘ä¸€å¹´æ•°æ®ï¼Œ
ç­›é€‰æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³å¯ï¼‰ï¼šæœ€è¿‘ 3 æ—¥å†…æœ‰æ—¥çº¿é‡‘å‰ã€æœ€è¿‘ 3 å‘¨å†…æœ‰å‘¨çº¿é‡‘å‰ã€æˆ– å‘¨çº¿ KDJ J < 5ã€‚
å¹¶å±•ç¤ºï¼šé‡‘å‰/æ­»å‰(æ—¥çº¿ã€å‘¨çº¿)ã€å‘¨çº¿å½¢æ€(talib)ã€æ—¥çº¿å½¢æ€(talib)ã€æˆäº¤é‡ã€KDJ(æ—¥)ã€KDJ(å‘¨)ã€‚
æ¯æ¬¡è¿è¡Œå‰è‡ªåŠ¨è°ƒç”¨ scripts/data_tools/update_data_history.py åšå¢é‡æ›´æ–°ï¼Œä¿è¯æ•°æ®å®Œæ•´æ€§ã€‚
å¤ç”¨ backtesting çš„ data_loaderã€indicatorsï¼Œä¸é‡å¤é€ è½®å­ã€‚
"""
import importlib.util
import json
import os
import sys
from datetime import date

import pandas as pd
import numpy as np

# é¡¹ç›®æ ¹ç›®å½•ï¼ˆbacktesting ä¸Šä¸€çº§ï¼‰ï¼Œä¾¿äºå¯¼å…¥ backtesting åŒ…
_REPO_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if _REPO_ROOT not in sys.path:
    sys.path.insert(0, _REPO_ROOT)

from backtesting.data_loader import load_data
from backtesting.notifier import TelegramNotifier
from backtesting.indicators import (
    calc_kdj,
    get_kdj_cross_signals,
    calc_weekly_kdj,
    resample_to_weekly,
)

DATA_DIR = os.path.join(_REPO_ROOT, "stock_data")
STOCK_LIST_PATH = os.path.join(DATA_DIR, "stock_list.json")
STOCK_NAMES_PATH = os.path.join(DATA_DIR, "stock_names.json")  # å¯é€‰ï¼šä»£ç  -> åç§°
REPORT_DIR = os.path.join(_REPO_ROOT, "watch_stock", "report")

# æœ€è¿‘ä¸€å¹´ï¼šæŒ‰è‡ªç„¶æ—¥å–çº¦ 365 å¤©ï¼ˆå®é™…ä¼šæŒ‰æ•°æ®é‡å–ï¼‰
LOOKBACK_DAYS = 365
# ç­›é€‰ï¼šæœ€è¿‘ 3 æ—¥å†…æœ‰æ—¥çº¿é‡‘å‰ æˆ– æœ€è¿‘ 3 å‘¨å†…æœ‰å‘¨çº¿é‡‘å‰ ä»»ä¸€å³å¯
LAST_N_DAYS = 3
LAST_N_WEEKS = 3
# KDJ é‡‘å‰/æ­»å‰ J é˜ˆå€¼
KDJ_J_THRESHOLD_LONG = 35   # é‡‘å‰ï¼šJ è¶…å–åŒº
KDJ_J_THRESHOLD_SHORT = 80  # æ­»å‰ï¼šJ è¶…ä¹°åŒº
# è¡¥å……è¿‡æ»¤ï¼šå‘¨çº¿ J < æ­¤å€¼è§†ä¸ºæè¶…å–ï¼Œä¹Ÿå¯é€šè¿‡
WEEKLY_J_OVERSOLD = 5

try:
    import talib
except ImportError:
    talib = None

def ensure_data_fresh():
    """
    ç›´æ¥è°ƒç”¨ scripts/data_tools/update_data_history.run_update å¢é‡æ›´æ–°æ—¥çº¿ CSVã€‚
    è‹¥ä¾èµ–ç¼ºå¤±ï¼ˆå¦‚ longportï¼‰æˆ–æ‰§è¡Œå¤±è´¥ï¼Œä»…æ‰“å°è­¦å‘Šå¹¶ç»§ç»­ï¼Œä¸é˜»å¡ä¸»æµç¨‹ã€‚
    """
    update_script = os.path.join(_REPO_ROOT, "scripts", "data_tools", "update_data_history.py")
    if not os.path.isfile(update_script):
        return
    print("æ­£åœ¨å¢é‡æ›´æ–°æ—¥çº¿æ•°æ®ï¼ˆupdate_data_historyï¼‰...")
    try:
        spec = importlib.util.spec_from_file_location("update_data_history", update_script)
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
        mod.run_update(DATA_DIR)
        print("æ•°æ®æ›´æ–°å®Œæˆã€‚")
    except Exception as e:
        print(f"[watch_metric] æ•°æ®æ›´æ–°å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨ç°æœ‰ CSV ç»§ç»­ã€‚")


def load_stock_list(path: str) -> list:
    """ä» stock_list.json è¯»å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ã€‚"""
    if not os.path.isfile(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data if isinstance(data, list) else []


def load_stock_names(path: str) -> dict:
    """ä» stock_names.json è¯»å–ä»£ç ->åç§°æ˜ å°„ã€‚è‹¥æ–‡ä»¶ä¸å­˜åœ¨æˆ–é dict åˆ™è¿”å› {}ã€‚"""
    if not os.path.isfile(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data if isinstance(data, dict) else {}
    except Exception:
        return {}


def _get_longport_credentials():
    """ä» mcp_config.json æˆ–ç¯å¢ƒå˜é‡è¯»å– LongPort å‡­è¯ï¼Œä¸ sync_watchlist_from_analysis ä¸€è‡´ã€‚"""
    config_path = os.path.expanduser("~/.gemini/antigravity/mcp_config.json")
    if not os.path.exists(config_path):
        return None
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception:
        return None
    env = {}
    if "mcpServers" in data and "longport-mcp" in data.get("mcpServers", {}):
        env = data["mcpServers"]["longport-mcp"].get("env", {})
    elif "longport-mcp" in data:
        env = data["longport-mcp"].get("env", data["longport-mcp"])
    app_key = env.get("LONGPORT_APP_KEY") or os.environ.get("LONGPORT_APP_KEY")
    app_secret = env.get("LONGPORT_APP_SECRET") or os.environ.get("LONGPORT_APP_SECRET")
    access_token = env.get("LONGPORT_ACCESS_TOKEN") or os.environ.get("LONGPORT_ACCESS_TOKEN")
    if not all([app_key, app_secret, access_token]):
        return None
    return app_key, app_secret, access_token


def fetch_stock_names_longport(symbols: list) -> dict:
    """
    é€šè¿‡ LongPort OpenAPI static_info è·å–ä»£ç ->åç§°æ˜ å°„ã€‚
    ä¼˜å…ˆä½¿ç”¨ name_cnï¼ˆç®€ä½“ï¼‰ï¼Œå¦åˆ™ name_enã€‚è‹¥æœªå®‰è£… longport æˆ–è¯·æ±‚å¤±è´¥åˆ™è¿”å› {}ã€‚
    """
    if not symbols:
        return {}
    try:
        from longport.openapi import QuoteContext, Config
    except ImportError:
        return {}
    creds = _get_longport_credentials()
    if not creds:
        return {}
    app_key, app_secret, access_token = creds
    try:
        cfg = Config(app_key, app_secret, access_token)
        ctx = QuoteContext(cfg)
        # å•æ¬¡æœ€å¤š 500 åªï¼ŒæŒ‰æ‰¹è¯·æ±‚
        batch = 200
        result = {}
        for i in range(0, len(symbols), batch):
            chunk = symbols[i : i + batch]
            infos = ctx.static_info(chunk)
            for item in infos:
                sym = getattr(item, "symbol", "") or ""
                name_cn = getattr(item, "name_cn", None) or ""
                name_en = getattr(item, "name_en", None) or ""
                name_hk = getattr(item, "name_hk", None) or ""
                result[sym] = (name_cn or name_hk or name_en or "-").strip() or "-"
        return result
    except Exception as e:
        print(f"[watch_metric] LongPort static_info è·å–åç§°å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ stock_names.json æˆ–ã€Œ-ã€ã€‚")
        return {}


def get_talib_pattern_names_at_bar(open_series, high_series, low_series, close_series, bar_index=-1):
    """
    åœ¨æŒ‡å®š K çº¿ä½ç½®æ£€æµ‹ TA-Lib å½¢æ€ï¼Œè¿”å›è¯¥ bar è§¦å‘çš„çœ‹å¤šå½¢æ€åç§°åˆ—è¡¨ã€‚
    bar_index: -1 è¡¨ç¤ºæœ€åä¸€æ ¹ã€‚
    """
    if talib is None:
        return []
    try:
        groups = talib.get_function_groups()
        pattern_names = groups.get("Pattern Recognition", [])
    except Exception:
        return []
    o = open_series.values.astype(float)
    h = high_series.values.astype(float)
    l = low_series.values.astype(float)
    c = close_series.values.astype(float)
    idx = bar_index if bar_index >= 0 else len(c) + bar_index
    if idx < 0 or idx >= len(c):
        return []
    names = []
    for name in pattern_names:
        if not hasattr(talib, name):
            continue
        try:
            func = getattr(talib, name)
            res = func(o, h, l, c)
            if res is not None and len(res) > idx and res[idx] == 100:
                names.append(name)
        except Exception:
            continue
    return names


def run():
    # 0. è¿è¡Œå‰ï¼šå¢é‡æ›´æ–°æ—¥çº¿æ•°æ®ï¼Œä¿è¯å®Œæ•´æ€§
    ensure_data_fresh()

    # 1. è¯»å–è‚¡ç¥¨åˆ—è¡¨
    symbols = load_stock_list(STOCK_LIST_PATH)
    if not symbols:
        print("stock_list.json ä¸ºç©ºæˆ–ä¸å­˜åœ¨ã€‚")
        return

    # 2. åŠ è½½æ—¥çº¿æ•°æ®ï¼ˆä»…åŠ è½½åˆ—è¡¨ä¸­çš„è‚¡ç¥¨ï¼‰
    data_map = load_data(DATA_DIR, symbols=symbols, period_suffix="day")
    if not data_map:
        print("æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ DATA_DIR ä¸‹æ˜¯å¦æœ‰å¯¹åº” CSVã€‚")
        return

    # 3. å–æœ€è¿‘ä¸€å¹´
    cutoff = pd.Timestamp.now() - pd.Timedelta(days=LOOKBACK_DAYS)
    data_map = {
        sym: df[df.index >= cutoff].copy()
        for sym, df in data_map.items()
        if len(df[df.index >= cutoff]) >= 50  # è‡³å°‘çº¦ 2 ä¸ªæœˆæ•°æ®
    }
    if not data_map:
        print("æˆªå–æœ€è¿‘ä¸€å¹´åæ— è¶³å¤Ÿæ•°æ®ã€‚")
        return

    print(f"å…± {len(data_map)} åªè‚¡ç¥¨å‚ä¸ç­›é€‰ï¼ˆæœ€è¿‘çº¦ä¸€å¹´æ•°æ®ï¼‰ã€‚")
    name_map = load_stock_names(STOCK_NAMES_PATH)
    # ä¼˜å…ˆç”¨ LongPort static_info æ‹‰å–åç§°ï¼Œè¦†ç›–/è¡¥å…¨ name_map
    symbols_list = list(data_map.keys())
    names_from_lp = fetch_stock_names_longport(symbols_list)
    if names_from_lp:
        name_map.update(names_from_lp)
        os.makedirs(os.path.dirname(STOCK_NAMES_PATH), exist_ok=True)
        try:
            with open(STOCK_NAMES_PATH, "w", encoding="utf-8") as f:
                json.dump(name_map, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

    # 4. ç­›é€‰ï¼šæœ€è¿‘ 3 æ—¥å†…æœ‰æ—¥çº¿é‡‘å‰ æˆ– æœ€è¿‘ 3 å‘¨å†…æœ‰å‘¨çº¿é‡‘å‰ ä»»ä¸€å³å¯
    results = []
    for sym, df in data_map.items():
        open_s = df["open"]
        high_s = df["high"]
        low_s = df["low"]
        close_s = df["close"]
        vol_s = df["volume"] if "volume" in df.columns else pd.Series(0.0, index=df.index)

        open_df = pd.DataFrame({sym: open_s})
        high_df = pd.DataFrame({sym: high_s})
        low_df = pd.DataFrame({sym: low_s})
        close_df = pd.DataFrame({sym: close_s})

        # æ—¥çº¿ KDJã€é‡‘å‰ã€æ­»å‰
        k, d, j = calc_kdj(close_s, high_s, low_s)
        daily_gold = get_kdj_cross_signals(k, d, j, threshold=KDJ_J_THRESHOLD_LONG, mode="long")
        daily_death = get_kdj_cross_signals(k, d, j, threshold=KDJ_J_THRESHOLD_SHORT, mode="short")
        last_3_days_gold = daily_gold.tail(LAST_N_DAYS).any()
        last_3_days_death = daily_death.tail(LAST_N_DAYS).any()

        # å‘¨çº¿ KDJ åœ¨å‘¨çº¿ç»´åº¦ä¸Šè®¡ç®—
        w_open, w_high, w_low, w_close = resample_to_weekly(open_df, high_df, low_df, close_df)
        wc, wh, wl = w_close[sym], w_high[sym], w_low[sym]
        wk_series, wd_series, wj_series = calc_kdj(wc, wh, wl, N=9, M=3)
        weekly_gold = get_kdj_cross_signals(wk_series, wd_series, wj_series, threshold=KDJ_J_THRESHOLD_LONG, mode="long")
        weekly_death = get_kdj_cross_signals(wk_series, wd_series, wj_series, threshold=KDJ_J_THRESHOLD_SHORT, mode="short")
        if len(weekly_gold) < LAST_N_WEEKS:
            continue
        last_3_weeks_gold = weekly_gold.tail(LAST_N_WEEKS).any()
        last_3_weeks_death = weekly_death.tail(LAST_N_WEEKS).any()
        weekly_j_last = wj_series.iloc[-1] if len(wj_series) else None

        # ç­›é€‰ï¼šæœ€è¿‘ 3 æ—¥æœ‰æ—¥çº¿é‡‘å‰ æˆ– æœ€è¿‘ 3 å‘¨æœ‰å‘¨çº¿é‡‘å‰ æˆ– å‘¨çº¿ J < 5
        if not (
            last_3_days_gold
            or last_3_weeks_gold
            or (pd.notna(weekly_j_last) and weekly_j_last < WEEKLY_J_OVERSOLD)
        ):
            continue

        # å‘¨çº¿ KDJ å¯¹é½åˆ°æ—¥çº¿ç´¢å¼•ï¼ˆç”¨äºå±•ç¤ºæœ€åä¸€æ—¥çš„ KDJ å‘¨ï¼‰
        dk, dd, dj = calc_weekly_kdj(open_df, high_df, low_df, close_df, N=9, M=3)
        wk, wd, wj = dk[sym], dd[sym], dj[sym]

        # å–æœ€è¿‘ä¸€æ ¹çš„å±•ç¤ºæ•°æ®
        last = df.index[-1]
        k_daily = k.iloc[-1] if not pd.isna(k.iloc[-1]) else None
        d_daily = d.iloc[-1] if not pd.isna(d.iloc[-1]) else None
        j_daily = j.iloc[-1] if not pd.isna(j.iloc[-1]) else None
        k_weekly = wk.iloc[-1] if not pd.isna(wk.iloc[-1]) else None
        d_weekly = wd.iloc[-1] if not pd.isna(wd.iloc[-1]) else None
        j_weekly = wj.iloc[-1] if not pd.isna(wj.iloc[-1]) else None
        volume_last = vol_s.iloc[-1] if len(vol_s) else None

        # æ—¥çº¿å½¢æ€ï¼šæœ€åä¸€æ ¹ K çº¿
        daily_patterns = get_talib_pattern_names_at_bar(open_s, high_s, low_s, close_s, -1)
        daily_pattern_str = ",".join(daily_patterns) if daily_patterns else "-"

        # å‘¨çº¿å½¢æ€ï¼šå…ˆ resample æˆå‘¨çº¿ï¼Œå†å–æœ€åä¸€æ ¹
        w_open, w_high, w_low, w_close = resample_to_weekly(
            open_df, high_df, low_df, close_df
        )
        if len(w_close) == 0:
            weekly_pattern_str = "-"
        else:
            wo = w_open[sym]
            wh = w_high[sym]
            wl = w_low[sym]
            wc = w_close[sym]
            weekly_patterns = get_talib_pattern_names_at_bar(wo, wh, wl, wc, -1)
            weekly_pattern_str = ",".join(weekly_patterns) if weekly_patterns else "-"

        # æ—¥çº¿ K/D/J + é‡‘å‰â¬†ï¸ï½œæ­»å‰â¬‡ï¸
        kdj_d = f"{round(k_daily, 2)}/{round(d_daily, 2)}/{round(j_daily, 2)}" if k_daily is not None and d_daily is not None and j_daily is not None else "-/-/-"
        cross_d = []
        if last_3_days_gold:
            cross_d.append("é‡‘å‰â¬†ï¸")
        if last_3_days_death:
            cross_d.append("æ­»å‰â¬‡ï¸")
        kdj_daily_str = f"{kdj_d} {'ï½œ'.join(cross_d)}" if cross_d else kdj_d

        # å‘¨çº¿ K/D/J + é‡‘å‰â¬†ï¸ï½œæ­»å‰â¬‡ï¸
        kdj_w = f"{round(k_weekly, 2)}/{round(d_weekly, 2)}/{round(j_weekly, 2)}" if k_weekly is not None and d_weekly is not None and j_weekly is not None else "-/-/-"
        cross_w = []
        if last_3_weeks_gold:
            cross_w.append("é‡‘å‰â¬†ï¸")
        if last_3_weeks_death:
            cross_w.append("æ­»å‰â¬‡ï¸")
        kdj_weekly_str = f"{kdj_w} {'ï½œ'.join(cross_w)}" if cross_w else kdj_w

        results.append({
            "è‚¡ç¥¨": sym,
            "åç§°": name_map.get(sym, "-"),
            "å‘¨çº¿å½¢æ€(talib)": weekly_pattern_str,
            "æ—¥çº¿å½¢æ€(talib)": daily_pattern_str,
            "æˆäº¤é‡": int(volume_last) if volume_last is not None and pd.notna(volume_last) else "-",
            "KDJ(æ—¥)": kdj_daily_str,
            "KDJ(å‘¨)": kdj_weekly_str,
        })

    # 5. è¾“å‡º Markdown åˆ° watch_stock/report/yyyy-mm-dd.md
    if not results:
        print("æ— ç¬¦åˆã€Œæœ€è¿‘ 3 æ—¥æ—¥çº¿é‡‘å‰ æˆ– æœ€è¿‘ 3 å‘¨å‘¨çº¿é‡‘å‰ æˆ– å‘¨çº¿ J<5ã€çš„è‚¡ç¥¨ã€‚")
        return

    out = pd.DataFrame(results)
    md_table = _df_to_markdown_table(out)
    os.makedirs(REPORT_DIR, exist_ok=True)
    report_name = date.today().strftime("%Y-%m-%d") + ".md"
    report_path = os.path.join(REPORT_DIR, report_name)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("## ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨\n\n")
        f.write(md_table)
        f.write("\n")
    print(f"æŠ¥å‘Šå·²å†™å…¥ï¼š{report_path}")

    # å¯é€‰ï¼šå‘é€ Telegram é€šçŸ¥ï¼ˆéœ€è®¾ç½®ç¯å¢ƒå˜é‡ TELEGRAM_TOKENã€TELEGRAM_CHAT_IDï¼‰
    _send_telegram_report(report_name, len(results), report_path)

    return out


def _send_telegram_report(report_name: str, count: int, report_path: str) -> None:
    """ä½¿ç”¨ backtesting.notifier.TelegramNotifier å°†æŠ¥å‘Šæ‘˜è¦å‘é€åˆ° Telegramã€‚"""
    token = os.environ.get("TELEGRAM_TOKEN")
    chat_id = os.environ.get("TELEGRAM_CHAT_ID")
    if not token or not chat_id:
        return
    notifier = TelegramNotifier(token=token, chat_id=chat_id)
    text = (
        f"<b>ğŸ“Š é€‰è‚¡æŠ¥å‘Šå·²ç”Ÿæˆ</b>\n\n"
        f"æ—¥æœŸ: {report_name.replace('.md', '')}\n"
        f"å…¥é€‰æ•°é‡: {count} åª\n"
        f"è·¯å¾„: <code>{report_path}</code>"
    )
    notifier.send_message(text)


def _df_to_markdown_table(df: pd.DataFrame) -> str:
    """å°† DataFrame è½¬ä¸º Markdown è¡¨æ ¼å­—ç¬¦ä¸²ã€‚"""
    cols = list(df.columns)
    lines = []
    # è¡¨å¤´
    lines.append("| " + " | ".join(str(c) for c in cols) + " |")
    lines.append("| " + " | ".join("---" for _ in cols) + " |")
    # æ•°æ®è¡Œ
    for _, row in df.iterrows():
        cells = [str(row[c]) if pd.notna(row[c]) else "" for c in cols]
        lines.append("| " + " | ".join(cells) + " |")
    return "\n".join(lines)


if __name__ == "__main__":
    run()
